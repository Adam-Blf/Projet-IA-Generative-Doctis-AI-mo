{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üè• Doctis AI - Fine-Tuning Notebook\n",
        "\n",
        "**Projet:** Doctis AI - Assistant de Pr√©-diagnostic M√©dical\n",
        "\n",
        "**Auteurs:** Adam Beloucif & Amina Medjdoub\n",
        "\n",
        "**Module:** IA G√©n√©rative & Data Engineering - EFREI 2025\n",
        "\n",
        "---\n",
        "\n",
        "Ce notebook permet de :\n",
        "1. Fine-tuner un SLM (Phi-3 / TinyLlama) sur des donn√©es m√©dicales\n",
        "2. Convertir le mod√®le en format GGUF pour ex√©cution CPU\n",
        "3. Pr√©parer le d√©ploiement sur Render"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Installation des d√©pendances"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Projet: Doctis AI\n",
        "# Auteurs: Adam Beloucif & Amina Medjdoub\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q transformers==4.38.0 datasets accelerate peft bitsandbytes trl\n",
        "!pip install -q huggingface_hub sentencepiece\n",
        "\n",
        "print(\"‚úÖ D√©pendances install√©es\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connexion √† Hugging Face (optionnel mais recommand√©)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# D√©commentez et ajoutez votre token HF\n",
        "# login(token=\"hf_VOTRE_TOKEN_ICI\")\n",
        "\n",
        "print(\"üí° Conseil: Ajoutez votre token HF pour acc√©der aux mod√®les priv√©s\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Configuration du mod√®le"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION - Modifiez selon vos besoins\n",
        "# =============================================================================\n",
        "\n",
        "# Choix du mod√®le de base\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # L√©ger, rapide\n",
        "# MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # Plus performant\n",
        "\n",
        "OUTPUT_DIR = \"./doctis-ai-finetuned\"\n",
        "\n",
        "print(f\"üìå Mod√®le s√©lectionn√©: {MODEL_NAME}\")\n",
        "print(f\"üìÅ R√©pertoire de sortie: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration de la quantification 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Chargement du mod√®le\n",
        "print(f\"‚è≥ Chargement de {MODEL_NAME}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Mod√®le charg√© avec succ√®s!\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Non disponible'}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Configuration LoRA"
      ],
      "metadata": {
        "id": "lora_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration LoRA pour un fine-tuning efficace\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                       # Rang des matrices LoRA\n",
        "    lora_alpha=32,              # Facteur d'√©chelle\n",
        "    target_modules=[            # Modules cibl√©s\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Pr√©paration du mod√®le pour l'entra√Ænement\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Affichage des statistiques\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Chargement du Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# OPTION 1: Dataset ChatDoctor (recommand√© pour le m√©dical)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"‚è≥ Chargement du dataset...\")\n",
        "\n",
        "# Charge un sous-ensemble pour l'entra√Ænement rapide\n",
        "dataset = load_dataset(\n",
        "    \"lavita/ChatDoctor-HealthCareMagic-100k\",\n",
        "    split=\"train[:3000]\"  # Ajustez selon vos besoins\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset charg√©: {len(dataset)} exemples\")\n",
        "print(f\"   Colonnes: {dataset.column_names}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatage des donn√©es pour l'entra√Ænement\n",
        "def format_medical_prompt(example):\n",
        "    \"\"\"Formate les exemples au format chat m√©dical.\"\"\"\n",
        "\n",
        "    # Extraction des champs (adapt√© au format ChatDoctor)\n",
        "    instruction = example.get(\"instruction\", example.get(\"input\", \"\"))\n",
        "    response = example.get(\"output\", example.get(\"response\", \"\"))\n",
        "\n",
        "    # Format de prompt pour Doctis AI\n",
        "    formatted_text = f\"\"\"<|system|>\n",
        "Tu es Doctis AI, un assistant m√©dical bienveillant et professionnel.\n",
        "Tu analyses les sympt√¥mes d√©crits par le patient et fournis des informations\n",
        "m√©dicales claires, rassurantes et fond√©es sur des connaissances m√©dicales.\n",
        "Tu rappelles toujours l'importance de consulter un professionnel de sant√©.<|end|>\n",
        "<|user|>\n",
        "{instruction}<|end|>\n",
        "<|assistant|>\n",
        "{response}<|end|>\"\"\"\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# Application du formatage\n",
        "dataset = dataset.map(format_medical_prompt)\n",
        "\n",
        "# Aper√ßu d'un exemple\n",
        "print(\"üìù Exemple de donn√©es format√©es:\")\n",
        "print(\"-\" * 50)\n",
        "print(dataset[0][\"text\"][:500] + \"...\")"
      ],
      "metadata": {
        "id": "format_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèãÔ∏è Entra√Ænement"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration de l'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,                    # Nombre d'√©poques\n",
        "    per_device_train_batch_size=4,         # Batch size\n",
        "    gradient_accumulation_steps=4,         # Accumulation de gradients\n",
        "    learning_rate=2e-4,                    # Taux d'apprentissage\n",
        "    weight_decay=0.01,                     # R√©gularisation\n",
        "    warmup_ratio=0.03,                     # Warmup\n",
        "    lr_scheduler_type=\"cosine\",            # Scheduler\n",
        "    logging_steps=25,                      # Fr√©quence de logging\n",
        "    save_steps=500,                        # Fr√©quence de sauvegarde\n",
        "    save_total_limit=2,                    # Nombre de checkpoints max\n",
        "    fp16=True,                             # Mixed precision\n",
        "    optim=\"paged_adamw_8bit\",              # Optimiseur efficace\n",
        "    report_to=\"none\",                      # Pas de logging externe\n",
        "    max_grad_norm=0.3,                     # Gradient clipping\n",
        ")\n",
        "\n",
        "# Cr√©ation du trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configur√©\")\n",
        "print(f\"   √âpoques: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size effectif: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ],
      "metadata": {
        "id": "training_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ LANCEMENT DE L'ENTRA√éNEMENT\n",
        "print(\"=\"*60)\n",
        "print(\"üè• DOCTIS AI - D√©marrage du fine-tuning\")\n",
        "print(\"   Auteurs: Adam Beloucif & Amina Medjdoub\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Entra√Ænement termin√©!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde du mod√®le fine-tun√©\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Mod√®le sauvegard√© dans {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÄ Fusion des poids LoRA"
      ],
      "metadata": {
        "id": "merge_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "print(\"‚è≥ Fusion des poids LoRA...\")\n",
        "\n",
        "# Recharge le mod√®le avec les adaptateurs\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    OUTPUT_DIR,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Fusion des poids\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "# Sauvegarde du mod√®le fusionn√©\n",
        "MERGED_DIR = \"./doctis-ai-merged\"\n",
        "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "print(f\"‚úÖ Mod√®le fusionn√© sauvegard√© dans {MERGED_DIR}\")"
      ],
      "metadata": {
        "id": "merge_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Conversion en GGUF"
      ],
      "metadata": {
        "id": "gguf_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation de llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"‚úÖ llama.cpp install√©\")"
      ],
      "metadata": {
        "id": "install_llamacpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion au format GGUF\n",
        "print(\"‚è≥ Conversion en GGUF (format F16)...\")\n",
        "\n",
        "!python convert_hf_to_gguf.py ../doctis-ai-merged \\\n",
        "    --outfile ../doctis-ai-f16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "print(\"‚úÖ Conversion F16 termin√©e\")"
      ],
      "metadata": {
        "id": "convert_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation de llama.cpp pour la quantification\n",
        "!make -j llama-quantize\n",
        "\n",
        "print(\"‚úÖ llama-quantize compil√©\")"
      ],
      "metadata": {
        "id": "compile_quantize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantification Q4_K_M (recommand√©e pour CPU)\n",
        "print(\"‚è≥ Quantification Q4_K_M...\")\n",
        "\n",
        "!./llama-quantize ../doctis-ai-f16.gguf ../doctis-ai-Q4_K_M.gguf Q4_K_M\n",
        "\n",
        "print(\"‚úÖ Quantification termin√©e\")\n",
        "\n",
        "# Affiche les tailles\n",
        "!ls -lh ../doctis-ai-*.gguf"
      ],
      "metadata": {
        "id": "quantize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• T√©l√©chargement du mod√®le"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"üì• T√©l√©chargement du mod√®le quantifi√©...\")\n",
        "print(\"   Cela peut prendre quelques minutes selon la taille du fichier.\")\n",
        "\n",
        "files.download(\"../doctis-ai-Q4_K_M.gguf\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ F√âLICITATIONS!\")\n",
        "print(\"   Votre mod√®le Doctis AI est pr√™t pour le d√©ploiement!\")\n",
        "print(\"   \")\n",
        "print(\"   Prochaine √©tape: Placez le fichier .gguf dans\")\n",
        "print(\"   le dossier backend/models/ de votre projet.\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n   Projet: Doctis AI\")\n",
        "print(\"   Auteurs: Adam Beloucif & Amina Medjdoub\")\n",
        "print(\"   EFREI 2025\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Test du mod√®le (Optionnel)"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test rapide du mod√®le GGUF avec llama-cpp-python\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "print(\"‚è≥ Chargement du mod√®le GGUF pour test...\")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"../doctis-ai-Q4_K_M.gguf\",\n",
        "    n_ctx=512,\n",
        "    n_threads=4,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Test de g√©n√©ration\n",
        "test_prompt = \"\"\"<|system|>\n",
        "Tu es Doctis AI, un assistant m√©dical bienveillant.<|end|>\n",
        "<|user|>\n",
        "J'ai mal au ventre en bas √† droite depuis ce matin, avec des naus√©es.<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüìù Test de g√©n√©ration:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "response = llm(\n",
        "    test_prompt,\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    stop=[\"<|end|>\", \"<|user|>\"]\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])\n",
        "print(\"-\" * 50)\n",
        "print(\"‚úÖ Test r√©ussi!\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
