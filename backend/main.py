# =============================================================================
# Projet: Doctis AI
# Auteurs: Adam Beloucif & Amina Medjdoub
# Description: API FastAPI pour le pr√©-diagnostic m√©dical avec RAG + LLM
# =============================================================================

"""
Doctis AI - Backend API
-----------------------
Architecture:
1. SBERT (all-MiniLM-L6-v2) pour le matching s√©mantique des sympt√¥mes
2. LLM quantiz√© (GGUF) pour la g√©n√©ration de r√©ponses empathiques
3. Base de donn√©es JSON de pathologies

Endpoints:
- POST /diagnose : Analyse des sympt√¥mes et pr√©-diagnostic
- GET /health : V√©rification de l'√©tat du service
- GET /pathologies : Liste des pathologies disponibles
"""

import json
import os
from pathlib import Path
from typing import Optional
from contextlib import asynccontextmanager

import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Tentative d'import de llama-cpp-python (optionnel si non install√©)
try:
    from llama_cpp import Llama
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    print("‚ö†Ô∏è  llama-cpp-python non install√©. Mode d√©grad√© activ√© (r√©ponses templates).")


# =============================================================================
# Configuration
# =============================================================================

class Settings:
    """Configuration de l'application."""
    APP_NAME: str = "Doctis AI"
    APP_VERSION: str = "1.0.0"
    AUTHORS: list = ["Adam Beloucif", "Amina Medjdoub"]

    # Chemins
    BASE_DIR: Path = Path(__file__).parent
    DATA_DIR: Path = BASE_DIR / "data"
    MODELS_DIR: Path = BASE_DIR / "models"

    # Mod√®les
    SBERT_MODEL: str = "all-MiniLM-L6-v2"
    LLM_MODEL_PATH: str = str(MODELS_DIR / "llama-3-8b-instruct.Q4_K_M.gguf")

    # Seuils
    SIMILARITY_THRESHOLD: float = 0.4
    LLM_MAX_TOKENS: int = 256
    LLM_TEMPERATURE: float = 0.7


settings = Settings()


# =============================================================================
# Mod√®les Pydantic (Sch√©mas)
# =============================================================================

class SymptomInput(BaseModel):
    """Sch√©ma d'entr√©e pour l'analyse des sympt√¥mes."""
    symptoms: str = Field(
        ...,
        min_length=10,
        max_length=2000,
        description="Description des sympt√¥mes en langage naturel",
        example="J'ai mal au ventre en bas √† droite et je vomis depuis ce matin"
    )


class PathologyMatch(BaseModel):
    """R√©sultat du matching d'une pathologie."""
    id: str
    name: str
    confidence_score: float = Field(..., ge=0.0, le=1.0)
    severity_level: int = Field(..., ge=1, le=5)
    urgency: str
    advice: str
    specialist: str


class DiagnosisResponse(BaseModel):
    """R√©ponse compl√®te du diagnostic."""
    success: bool
    matched: bool
    pathology: Optional[PathologyMatch] = None
    ai_response: str
    disclaimer: str
    authors: list[str] = settings.AUTHORS


class HealthResponse(BaseModel):
    """R√©ponse du health check."""
    status: str
    app_name: str
    version: str
    models_loaded: dict
    authors: list[str]


# =============================================================================
# Services
# =============================================================================

class DoctisAIService:
    """
    Service principal de Doctis AI.
    G√®re le chargement des mod√®les et l'analyse des sympt√¥mes.
    """

    def __init__(self):
        self.sbert_model: Optional[SentenceTransformer] = None
        self.llm_model: Optional[Llama] = None
        self.pathologies: list = []
        self.pathology_embeddings: Optional[np.ndarray] = None
        self.disclaimer: str = ""

    def load_pathologies(self) -> None:
        """Charge la base de donn√©es des pathologies."""
        pathologies_path = settings.DATA_DIR / "pathologies.json"

        if not pathologies_path.exists():
            raise FileNotFoundError(f"Fichier pathologies.json non trouv√©: {pathologies_path}")

        with open(pathologies_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.pathologies = data.get("pathologies", [])
        self.disclaimer = data.get("disclaimer", "Consultez un m√©decin pour tout diagnostic m√©dical.")

        print(f"‚úÖ {len(self.pathologies)} pathologies charg√©es")

    def load_sbert_model(self) -> None:
        """Charge le mod√®le SBERT pour les embeddings."""
        print(f"‚è≥ Chargement du mod√®le SBERT: {settings.SBERT_MODEL}...")
        self.sbert_model = SentenceTransformer(settings.SBERT_MODEL)
        print("‚úÖ Mod√®le SBERT charg√©")

        # Pr√©-calcul des embeddings des pathologies
        if self.pathologies:
            symptoms_texts = [p["symptoms_description"] for p in self.pathologies]
            self.pathology_embeddings = self.sbert_model.encode(symptoms_texts)
            print(f"‚úÖ Embeddings pr√©-calcul√©s pour {len(symptoms_texts)} pathologies")

    def load_llm_model(self) -> None:
        """Charge le mod√®le LLM quantiz√© (GGUF)."""
        if not LLAMA_AVAILABLE:
            print("‚ö†Ô∏è  LLM non disponible (llama-cpp-python non install√©)")
            return

        if not os.path.exists(settings.LLM_MODEL_PATH):
            print(f"‚ö†Ô∏è  Fichier LLM non trouv√©: {settings.LLM_MODEL_PATH}")
            print("   ‚Üí Mode d√©grad√© activ√© (r√©ponses templates)")
            return

        print(f"‚è≥ Chargement du mod√®le LLM: {settings.LLM_MODEL_PATH}...")
        self.llm_model = Llama(
            model_path=settings.LLM_MODEL_PATH,
            n_ctx=2048,
            n_threads=4,
            verbose=False
        )
        print("‚úÖ Mod√®le LLM charg√©")

    def compute_similarity(self, user_input: str) -> list[tuple[dict, float]]:
        """
        Calcule la similarit√© cosinus entre l'input utilisateur
        et les descriptions de sympt√¥mes des pathologies.

        Returns:
            Liste de tuples (pathologie, score) tri√©e par score d√©croissant
        """
        if self.sbert_model is None or self.pathology_embeddings is None:
            raise RuntimeError("Mod√®le SBERT non initialis√©")

        # Encode l'input utilisateur
        user_embedding = self.sbert_model.encode([user_input])

        # Calcule la similarit√© cosinus
        similarities = cosine_similarity(user_embedding, self.pathology_embeddings)[0]

        # Associe chaque pathologie √† son score
        results = list(zip(self.pathologies, similarities))

        # Trie par score d√©croissant
        results.sort(key=lambda x: x[1], reverse=True)

        return results

    def generate_llm_response(self, pathology: dict, confidence: float) -> str:
        """
        G√©n√®re une r√©ponse empathique via le LLM.
        Si le LLM n'est pas disponible, utilise un template.
        """
        if self.llm_model is None:
            # Mode d√©grad√© : template de r√©ponse
            return self._generate_template_response(pathology, confidence)

        # Prompt syst√®me m√©dical
        system_prompt = """Tu es Doctis AI, un assistant m√©dical bienveillant et professionnel.
Tu dois g√©n√©rer une r√©ponse empathique pour un patient qui pr√©sente des sympt√¥mes.
Reste rassurant mais prudent. Rappelle toujours l'importance de consulter un m√©decin.
R√©ponds en fran√ßais, de mani√®re concise (2-3 phrases maximum)."""

        user_prompt = f"""Le patient pr√©sente des sympt√¥mes correspondant √† : {pathology['name']}
Niveau de confiance de l'analyse : {confidence*100:.0f}%
Gravit√© : {pathology['severity_level']}/5
Conseil m√©dical de base : {pathology['advice']}

G√©n√®re une r√©ponse empathique et rassurante pour le patient."""

        # G√©n√©ration via LLM
        full_prompt = f"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{user_prompt}<|end|>\n<|assistant|>\n"

        response = self.llm_model(
            full_prompt,
            max_tokens=settings.LLM_MAX_TOKENS,
            temperature=settings.LLM_TEMPERATURE,
            stop=["<|end|>", "<|user|>"]
        )

        return response["choices"][0]["text"].strip()

    def _generate_template_response(self, pathology: dict, confidence: float) -> str:
        """G√©n√®re une r√©ponse template si le LLM n'est pas disponible."""
        severity_messages = {
            1: "Il s'agit g√©n√©ralement d'une condition b√©nigne.",
            2: "Cette condition est g√©n√©ralement mod√©r√©e et g√©rable.",
            3: "Cette condition m√©rite une attention m√©dicale.",
            4: "Cette condition n√©cessite une consultation m√©dicale rapide.",
            5: "Cette condition peut √™tre s√©rieuse et n√©cessite une attention m√©dicale urgente."
        }

        severity_msg = severity_messages.get(pathology['severity_level'], "")

        response = f"""D'apr√®s l'analyse de vos sympt√¥mes, je d√©tecte une possible **{pathology['name']}** avec un niveau de confiance de {confidence*100:.0f}%.

{severity_msg}

**Recommandation :** {pathology['advice']}

N'oubliez pas : seul un professionnel de sant√© peut √©tablir un diagnostic d√©finitif. Si vos sympt√¥mes persistent ou s'aggravent, consultez rapidement un m√©decin."""

        return response

    def diagnose(self, symptoms: str) -> DiagnosisResponse:
        """
        Effectue le pr√©-diagnostic complet.

        Args:
            symptoms: Description des sympt√¥mes par l'utilisateur

        Returns:
            DiagnosisResponse avec le r√©sultat du diagnostic
        """
        # Calcul de similarit√©
        matches = self.compute_similarity(symptoms)

        if not matches:
            return DiagnosisResponse(
                success=True,
                matched=False,
                ai_response="Je n'ai pas pu analyser vos sympt√¥mes. Veuillez reformuler votre description ou consulter un m√©decin.",
                disclaimer=self.disclaimer
            )

        # Prend le meilleur match
        best_match, best_score = matches[0]

        # V√©rifie le seuil de similarit√©
        if best_score < settings.SIMILARITY_THRESHOLD:
            return DiagnosisResponse(
                success=True,
                matched=False,
                ai_response=f"Vos sympt√¥mes ne correspondent pas de mani√®re suffisamment pr√©cise √† une pathologie connue dans ma base de donn√©es (score: {best_score*100:.0f}%). Je vous recommande de consulter un m√©decin pour une √©valuation personnalis√©e.",
                disclaimer=self.disclaimer
            )

        # G√©n√®re la r√©ponse IA
        ai_response = self.generate_llm_response(best_match, best_score)

        # Construit la r√©ponse
        pathology_match = PathologyMatch(
            id=best_match["id"],
            name=best_match["name"],
            confidence_score=round(float(best_score), 3),
            severity_level=best_match["severity_level"],
            urgency=best_match["urgency"],
            advice=best_match["advice"],
            specialist=best_match["specialist"]
        )

        return DiagnosisResponse(
            success=True,
            matched=True,
            pathology=pathology_match,
            ai_response=ai_response,
            disclaimer=self.disclaimer
        )


# =============================================================================
# Application FastAPI
# =============================================================================

# Instance globale du service
doctis_service = DoctisAIService()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application."""
    # Startup
    print("\n" + "="*60)
    print("üè• D√©marrage de Doctis AI")
    print(f"   Auteurs: {', '.join(settings.AUTHORS)}")
    print("="*60 + "\n")

    doctis_service.load_pathologies()
    doctis_service.load_sbert_model()
    doctis_service.load_llm_model()

    print("\n‚úÖ Doctis AI pr√™t √† recevoir des requ√™tes!\n")

    yield

    # Shutdown
    print("\nüëã Arr√™t de Doctis AI\n")


# Cr√©ation de l'application
app = FastAPI(
    title="Doctis AI",
    description="API de pr√©-diagnostic m√©dical utilisant l'IA (RAG + LLM)",
    version=settings.APP_VERSION,
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =============================================================================
# Endpoints
# =============================================================================

@app.get("/", tags=["Info"])
async def root():
    """Page d'accueil de l'API."""
    return {
        "message": "Bienvenue sur Doctis AI - API de pr√©-diagnostic m√©dical",
        "version": settings.APP_VERSION,
        "authors": settings.AUTHORS,
        "documentation": "/docs",
        "endpoints": {
            "diagnose": "POST /diagnose",
            "health": "GET /health",
            "pathologies": "GET /pathologies"
        }
    }


@app.get("/health", response_model=HealthResponse, tags=["Syst√®me"])
async def health_check():
    """V√©rifie l'√©tat de sant√© du service."""
    return HealthResponse(
        status="healthy",
        app_name=settings.APP_NAME,
        version=settings.APP_VERSION,
        models_loaded={
            "sbert": doctis_service.sbert_model is not None,
            "llm": doctis_service.llm_model is not None,
            "pathologies_count": len(doctis_service.pathologies)
        },
        authors=settings.AUTHORS
    )


@app.get("/pathologies", tags=["Donn√©es"])
async def get_pathologies():
    """Retourne la liste des pathologies disponibles."""
    return {
        "count": len(doctis_service.pathologies),
        "pathologies": [
            {
                "id": p["id"],
                "name": p["name"],
                "severity_level": p["severity_level"]
            }
            for p in doctis_service.pathologies
        ],
        "authors": settings.AUTHORS
    }


@app.post("/diagnose", response_model=DiagnosisResponse, tags=["Diagnostic"])
async def diagnose(input_data: SymptomInput):
    """
    Analyse les sympt√¥mes et retourne un pr√©-diagnostic.

    Le syst√®me utilise:
    1. SBERT pour le matching s√©mantique avec les pathologies connues
    2. LLM pour g√©n√©rer une r√©ponse empathique et personnalis√©e

    **‚ö†Ô∏è Disclaimer:** Ce service est fourni √† titre informatif uniquement
    et ne remplace pas une consultation m√©dicale professionnelle.
    """
    try:
        result = doctis_service.diagnose(input_data.symptoms)
        return result
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de l'analyse: {str(e)}"
        )


# =============================================================================
# Point d'entr√©e
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
